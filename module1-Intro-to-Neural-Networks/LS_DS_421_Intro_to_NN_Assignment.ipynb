{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVfaLrjLvxvQ"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Neural Networks\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 1*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "## Define the Following:\n",
    "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
    "\n",
    "### Input Layer:\n",
    "\n",
    "Recieves input from the dataset. The 'first' or leftmost layer in a neural network. Typically node maps are drawn with one input for each of the different inputs/featur        es/columns in a dataset. \n",
    "\n",
    "### Hidden Lay\n",
    "\n",
    "Refers to all the layers between the input and output layers. \"Deep learning\" effectively refers to using a neural network that has muliple hidden layers.\n",
    "### Output Layer:\n",
    "\n",
    "The final ('rightmost') layer. Typically the output layer outputs a vector of values in a format suitable for the type of problem to be addressed. Often the output value is modified by an 'activation function' to transform it into a format that makes sense for a given context.\n",
    "\n",
    "### Neuron:\n",
    "\n",
    "A neuron (aka. a node) takes all the inputs, performs some action (typically using the weights) and then applies an activation function (typically the function is used within one layer) and produces an output.\n",
    "\n",
    "### Weight:\n",
    "\n",
    "Weights are the coefficients that inputs are multiplied by within a neuron. Optimal weights can be found through gradient descent if there is a loss function that evaluates the quality of our predictions compared to the targets in the training data.\n",
    "\n",
    "### Activation Function:\n",
    "\n",
    "Activation functions decide how much signal is passed onto the next layer (how on or off the node is). Each node in a given layer typically has the same activation function. Activation functions are sometimes referred to as transfer functions because they determine how much signal is transferred to the next layer.\n",
    "\n",
    "### Node Map:\n",
    "\n",
    "\n",
    "A node map is a simple diagram which visually explains how a node within a neural network is computed. Node maps typically show the input, hidden, output layers, and the connections between each layer.\n",
    "\n",
    "### Perceptron:\n",
    "\n",
    "A percepron is a single node or neuron of a neural network. A perceptron takes any number of inputs and produces an output. Essentially a neuron takes each input values, multiplies it by a weight, sums all the products, and passes the sum through an activation function which produces the final value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXuy9WcWzxa4"
   },
   "source": [
    "## Inputs -> Outputs\n",
    "\n",
    "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlSwIJMC0A8F"
   },
   "source": [
    "#### Your Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6sWR43PTwhSk"
   },
   "source": [
    "## Write your own perceptron code that can correctly classify (99.0% accuracy) a NAND gate. \n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = { 'x1': [0,1,0,1],\n",
    "         'x2': [0,0,1,1],\n",
    "         'y':  [1,1,1,0]\n",
    "       }\n",
    "\n",
    "df = pd.DataFrame.from_dict(data).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0, 0],\n       [1, 0],\n       [0, 1],\n       [1, 1]])"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "##### Your Code Here #####\n",
    "X = df.loc[:,['x1','x2']].values\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))\n",
    "\n",
    "# used to update weights\n",
    "def sigmoid_derivate(x):\n",
    "    sx = sigmoid(x)\n",
    "    return sx * (1 - sx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[ 0.07126719],\n       [-0.19195602]])"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "weights = 2 * np.random.random((2,1)) - 1\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[ 0.        ],\n       [ 0.07126719],\n       [-0.19195602],\n       [-0.12068882]])"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "weighted_sum = np.dot(X, weights)\n",
    "weighted_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0.5       ],\n       [0.51780926],\n       [0.45215781],\n       [0.46986436]])"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "activated_output = sigmoid(weighted_sum)\n",
    "activated_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[[1], [1], [1], [0]]"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "correct = df['y'].values\n",
    "correct = [[i] for i in correct]\n",
    "correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[ 0.5       ],\n       [ 0.48219074],\n       [ 0.54784219],\n       [-0.46986436]])"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "error = correct - activated_output\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[ 0.11750186],\n       [ 0.11281503],\n       [ 0.13019207],\n       [-0.11121407]])"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "adjustments = error * sigmoid_derivate(activated_output)\n",
    "adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights += np.dot(X.T, adjustments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Weights after training\n[[ 1.24900090e-16]\n [-3.05311332e-16]]\nOutput after training\n[[0.5]\n [0.5]\n [0.5]\n [0.5]]\n"
    }
   ],
   "source": [
    "for i in range(10000):\n",
    "    weighted_sum = np.dot(X, weights)\n",
    "    activated_output = sigmoid(weighted_sum)\n",
    "    error = correct - activated_output\n",
    "    adjustments = error * sigmoid_derivate(activated_output)\n",
    "    weights += np.dot(X.T, adjustments)\n",
    "    \n",
    "print(\"Weights after training\")\n",
    "print(weights)\n",
    "\n",
    "print(\"Output after training\")\n",
    "print(activated_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf7sdqVs0s4x"
   },
   "source": [
    "## Implement your own Perceptron Class and use it to classify a binary dataset: \n",
    "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
    "\n",
    "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n0            6      148             72             35        0  33.6   \n1            1       85             66             29        0  26.6   \n2            8      183             64              0        0  23.3   \n3            1       89             66             23       94  28.1   \n4            0      137             40             35      168  43.1   \n\n   DiabetesPedigreeFunction  Age  Outcome  \n0                     0.627   50        1  \n1                     0.351   31        0  \n2                     0.672   32        1  \n3                     0.167   21        0  \n4                     2.288   33        1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pregnancies</th>\n      <th>Glucose</th>\n      <th>BloodPressure</th>\n      <th>SkinThickness</th>\n      <th>Insulin</th>\n      <th>BMI</th>\n      <th>DiabetesPedigreeFunction</th>\n      <th>Age</th>\n      <th>Outcome</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6</td>\n      <td>148</td>\n      <td>72</td>\n      <td>35</td>\n      <td>0</td>\n      <td>33.6</td>\n      <td>0.627</td>\n      <td>50</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>85</td>\n      <td>66</td>\n      <td>29</td>\n      <td>0</td>\n      <td>26.6</td>\n      <td>0.351</td>\n      <td>31</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8</td>\n      <td>183</td>\n      <td>64</td>\n      <td>0</td>\n      <td>0</td>\n      <td>23.3</td>\n      <td>0.672</td>\n      <td>32</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>89</td>\n      <td>66</td>\n      <td>23</td>\n      <td>94</td>\n      <td>28.1</td>\n      <td>0.167</td>\n      <td>21</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>137</td>\n      <td>40</td>\n      <td>35</td>\n      <td>168</td>\n      <td>43.1</td>\n      <td>2.288</td>\n      <td>33</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "diabetes = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv')\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although neural networks can handle non-normalized data, scaling or normalizing your data will improve your neural network's learning speed. Try to apply the sklearn `MinMaxScaler` or `Normalizer` to your diabetes dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[  6.   , 148.   ,  72.   , ...,  33.6  ,   0.627,  50.   ],\n       [  1.   ,  85.   ,  66.   , ...,  26.6  ,   0.351,  31.   ],\n       [  8.   , 183.   ,  64.   , ...,  23.3  ,   0.672,  32.   ],\n       ...,\n       [  5.   , 121.   ,  72.   , ...,  26.2  ,   0.245,  30.   ],\n       [  1.   , 126.   ,  60.   , ...,  30.1  ,   0.349,  47.   ],\n       [  1.   ,  93.   ,  70.   , ...,  30.4  ,   0.315,  23.   ]])"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "\n",
    "feats = list(diabetes)[:-1]\n",
    "feats\n",
    "\n",
    "X = diabetes.drop(['Outcome'], axis=1)\n",
    "\n",
    "X_val = X.values\n",
    "X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-23-4174a8cf5009>, line 20)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-23-4174a8cf5009>\"\u001b[1;36m, line \u001b[1;32m20\u001b[0m\n\u001b[1;33m    \"\"\"\u001b[0m\n\u001b[1;37m       \n^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "##### Update this Class #####\n",
    "\n",
    "class Perceptron():\n",
    "    \n",
    "    def __init__(self, niter = 10):\n",
    "        self.niter = niter\n",
    "    \n",
    "    def __sigmoid(self, x):\n",
    "        return 1 / (1+np.exp(-s))\n",
    "\n",
    "    \n",
    "    def __sigmoid_derivative(self, x):\n",
    "        sx = self.sigmoid(s)\n",
    "        return sx * (1-sx)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "    \"\"\"Fit training data\n",
    "    X : Training vectors, X.shape : [#samples, #features]\n",
    "    y : Target values, y.shape : [#samples]\n",
    "    \"\"\"\n",
    "    # Add bias in term\n",
    "    X = np.append(X_, np.ones(X_[0]), axis=1)\n",
    "    \n",
    "    \n",
    "    # Randomly Initialize Weights\n",
    "    self.weights = np.random.random((X.shape[1],1))\n",
    "\n",
    "    for i in range(self.niter):\n",
    "        # Weighted sum of inputs / weights\n",
    "        weighted_sum = np.dot(X, )\n",
    "        # Activate!\n",
    "\n",
    "        # Cac error\n",
    "\n",
    "        # Update the Weights\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "    \"\"\"Return class label after unit step\"\"\"\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QR4oAW1xdyu"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
    "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
    "- Try and implement your own backpropagation algorithm.\n",
    "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_431_Intro_to_NN_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "U4-S2-NN (Python3)",
   "language": "python",
   "name": "u4-s2-nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}